{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from functools import partial\n",
    "DEVICE_COUNT = 8\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count={}\".format(DEVICE_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit\n",
    "import haiku as hk\n",
    "from model import Linear, RMSNorm, DenseFF, RotaryEmbedding, MultiHeadAttention, MoEBlock, TransformerBlock, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "seq_len = 200\n",
    "emd_dim = 256\n",
    "num_heads = 8\n",
    "k_dim = emd_dim //num_heads\n",
    "v_dim = emd_dim //num_heads\n",
    "x = random.normal(random.key(1), (batch_size, seq_len, emd_dim))\n",
    "heads = random.normal(random.key(1), (batch_size, seq_len, num_heads, k_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_module(mod, configs, loss_fn=None):\n",
    "  def compute_loss(*args, **kwargs):\n",
    "    m = mod(**configs)\n",
    "    y = m(*args, **kwargs)\n",
    "\n",
    "    if loss_fn is None:\n",
    "      return jnp.mean(y[0])\n",
    "    else:\n",
    "      return loss_fn(y)\n",
    "\n",
    "  def f(*args, **kwargs):\n",
    "    m = mod(**configs)\n",
    "    y = m(*args, **kwargs)\n",
    "\n",
    "    g = jax.grad(compute_loss)(*args, **kwargs)\n",
    "    return y, g\n",
    "  \n",
    "  f = hk.transform_with_state(f)\n",
    "  return f\n",
    "\n",
    "def test_jit(f, *args, **kwargs):\n",
    "  key = random.PRNGKey(1)\n",
    "  params, state = f.init(key, *args, **kwargs)\n",
    "  start = time.perf_counter()\n",
    "  (y1, g1), state = f.apply(params, state, key, *args, **kwargs)\n",
    "  end = time.perf_counter()\n",
    "  non_jitted_latency = end - start\n",
    "\n",
    "  params, state = jit(f.init)(key, *args, **kwargs)\n",
    "  jitted = jit(f.apply) \n",
    "  _ = jitted(params, state, key, *args, **kwargs)\n",
    "  start = time.perf_counter()\n",
    "  (y2, g2), state = jitted(params, state, key, *args, **kwargs)\n",
    "  end = time.perf_counter()\n",
    "  jitted_latency = end - start\n",
    "\n",
    "  y1_leaves = jax.tree_util.tree_leaves(y1) \n",
    "  y2_leaves = jax.tree_util.tree_leaves(y2)\n",
    "  g1_leaves = jax.tree_util.tree_leaves(g1)\n",
    "  g2_leaves = jax.tree_util.tree_leaves(g2)\n",
    "\n",
    "  for i in range(len(y1_leaves)):\n",
    "    y_abs_err = jnp.abs(y1_leaves[i] - y2_leaves[i])\n",
    "    y_rel_err = jnp.abs(2 * (y1_leaves[i] - y2_leaves[i]) / (y1_leaves[i] + y2_leaves[i]))\n",
    "    print('y_abs_err: {} \\t y_rel_err: {}'.format(\n",
    "      jnp.mean(y_abs_err), jnp.mean(y_rel_err)\n",
    "    ))\n",
    "\n",
    "  for i in range(len(g1_leaves)):\n",
    "    g_abs_err = jnp.abs(g1_leaves[i] - g2_leaves[i])\n",
    "    g_rel_err = jnp.abs(2 * (g1_leaves[i] - g2_leaves[i]) / (g1_leaves[i] + g2_leaves[i]))\n",
    "    print('g_abs_err: {} \\t g_rel_err: {}'.format(\n",
    "      jnp.mean(g_abs_err),  jnp.mean(g_rel_err)\n",
    "    ))\n",
    "  \n",
    "  print('latency: {} \\t jitted: {}'.format(non_jitted_latency, jitted_latency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear =  transform_module(Linear, {'in_dim': emd_dim, 'out_dim': emd_dim }) \n",
    "test_jit(linear, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_norm =  transform_module(RMSNorm, {}) \n",
    "test_jit(rms_norm, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = transform_module(DenseFF, {'emd_dim': emd_dim, 'activation': 'gelu', 'hidden_dim': emd_dim * 2})\n",
    "test_jit(ff, x)\n",
    "ff = transform_module(DenseFF, {'emd_dim': emd_dim, 'activation': 'silu', 'hidden_dim': emd_dim * 2})\n",
    "test_jit(ff, x)\n",
    "ff = transform_module(DenseFF, {'emd_dim': emd_dim, 'activation': 'relu', 'hidden_dim': emd_dim * 2})\n",
    "test_jit(ff, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rote = transform_module(RotaryEmbedding, {'dim': k_dim})\n",
    "test_jit(rote, heads, offset=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MultiHeadAttention\n",
    "mha = transform_module(MultiHeadAttention, {\n",
    "          'emd_dim': emd_dim,\n",
    "          'num_q_heads': num_heads,\n",
    "          'num_kv_heads': num_heads,\n",
    "          'v_dim': v_dim,\n",
    "          'k_dim': k_dim,\n",
    "})\n",
    "test_jit(mha, x, x, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = hk.initializers.TruncatedNormal(stddev=1)\n",
    "mha2 = transform_module(hk.MultiHeadAttention, {\n",
    "          'num_heads': num_heads,\n",
    "          'key_size': k_dim,\n",
    "          'model_size': emd_dim,\n",
    "          'w_init': initializer\n",
    "})\n",
    "test_jit(mha2, x, x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moe_block = transform_module(MoEBlock, {\n",
    "    'emd_dim': emd_dim,\n",
    "    'hidden_dim': emd_dim * 4,\n",
    "    'num_experts': 8,\n",
    "    'active_experts': 1,\n",
    "    'multi_device': True\n",
    "\n",
    "})\n",
    "test_jit(moe_block, x)\n",
    "\n",
    "moe_block = transform_module(MoEBlock, {\n",
    "    'emd_dim': emd_dim,\n",
    "    'hidden_dim': emd_dim * 4,\n",
    "    'num_experts': 1,\n",
    "    'active_experts': 1,\n",
    "    'multi_device': True\n",
    "\n",
    "})\n",
    "test_jit(moe_block, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_embedding(mod, configs):\n",
    "  def encode(*args, **kwargs):\n",
    "    m = mod(**configs)\n",
    "    y = m.encode(*args, **kwargs)\n",
    "    g = jax.grad(\n",
    "      lambda *args, **kwargs: jnp.mean(m.encode(*args, **kwargs))\n",
    "    )(*args, **kwargs)\n",
    "\n",
    "    return y, g\n",
    "  \n",
    "  def decode(*args, **kwargs):\n",
    "    m = mod(**configs)\n",
    "    y = m.decode(*args, **kwargs)\n",
    "    g = jax.grad(\n",
    "      lambda *args, **kwargs: jnp.mean(m.decode(*args, **kwargs))\n",
    "    )(*args, **kwargs)\n",
    "\n",
    "    return y, g\n",
    "  \n",
    "  encode = hk.transform_with_state(encode)\n",
    "  decode = hk.transform_with_state(decode)\n",
    "  return encode, decode\n",
    "\n",
    "emd_encode, emd_decode = transform_embedding(Embedding, {\n",
    "    'emd_dim': emd_dim,\n",
    "    'n_vocab': emd_dim,\n",
    "})\n",
    "test_jit(emd_encode, x)\n",
    "test_jit(emd_decode, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import TransformerBlock\n",
    "transformer_block =  transform_module(TransformerBlock, {\n",
    "    'emd_dim': emd_dim,\n",
    "    'num_q_heads': num_heads,\n",
    "    'num_kv_heads': num_heads,\n",
    "    'v_dim': v_dim,\n",
    "    'k_dim': k_dim,\n",
    "    'hidden_dim': emd_dim * 4, \n",
    "    'num_experts': 1,\n",
    "    'active_experts': 1,\n",
    "    'expert_capacity': 1.0\n",
    "})\n",
    "test_jit(transformer_block, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MoeTransformer\n",
    "\n",
    "transformer = transform_module(MoeTransformer, {\n",
    "    'depth': 5,\n",
    "    'n_vocab': emd_dim,\n",
    "    'emd_dim': emd_dim,\n",
    "    'num_q_heads': num_heads,\n",
    "    'num_kv_heads': num_heads,\n",
    "    'v_dim': v_dim,\n",
    "    'k_dim': k_dim,\n",
    "    'hidden_dim': emd_dim * 4, \n",
    "    'num_experts': 1,\n",
    "    'active_experts': 1,\n",
    "    'expert_capacity': 1.0\n",
    "})\n",
    "test_jit(transformer, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _layer_norm(x: jax.Array) -> jax.Array:\n",
    "  \"\"\"Applies a unique LayerNorm to `x` with default settings.\"\"\"\n",
    "  ln = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "  return ln(x)\n",
    "\n",
    "  \n",
    "initializer = hk.initializers.TruncatedNormal(stddev=1)\n",
    "def transformer(h):\n",
    "    for _ in range(5):\n",
    "      # First the attention block.\n",
    "      attn_block = hk.MultiHeadAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_size=k_dim,\n",
    "          model_size=emd_dim,\n",
    "          w_init=initializer,\n",
    "      )\n",
    "      h_norm = h\n",
    "      h_attn = attn_block(h_norm, h_norm, h_norm, None)\n",
    "      # h_attn = hk.dropout(hk.next_rng_key(), 0.1, h_attn)\n",
    "      h = h + h_attn\n",
    "\n",
    "      # Then the dense block.\n",
    "      dense_block = hk.Sequential([\n",
    "          hk.Linear(4 * emd_dim, w_init=initializer),\n",
    "          jax.nn.gelu,\n",
    "          hk.Linear(emd_dim, w_init=initializer),\n",
    "      ])\n",
    "      h_norm = _layer_norm(h)\n",
    "      h_dense = dense_block(h_norm)\n",
    "      h_dense = hk.dropout(hk.next_rng_key(), 0.2, h_dense)\n",
    "      h = h + h_dense\n",
    "\n",
    "    return jnp.mean(h)\n",
    "    \n",
    "transformer = hk.transform(transformer)\n",
    "params = transformer.init(random.PRNGKey(1), x)\n",
    "\n",
    "jitted = jit(transformer.apply)\n",
    "_  = jitted(params, random.PRNGKey(1), x)\n",
    "\n",
    "jitted_grad = jit(jax.grad(transformer.apply))\n",
    "_ = jitted_grad(params, random.PRNGKey(1), x)\n",
    "\n",
    "start = time.perf_counter()\n",
    "y = jitted(params, random.PRNGKey(1), x)\n",
    "end = time.perf_counter()\n",
    "jitted_latency = end - start\n",
    "print(jitted_latency)\n",
    "\n",
    "start = time.perf_counter()\n",
    "grads = jitted_grad(params, random.PRNGKey(1), x)\n",
    "end = time.perf_counter()\n",
    "grad_latency = end - start\n",
    "print(grad_latency)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import mesh_utils\n",
    "from jax.sharding import PositionalSharding\n",
    "devices = mesh_utils.create_device_mesh((8,))\n",
    "sharding = PositionalSharding(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jax.random.normal(jax.random.key(0), (8192, 8192))\n",
    "# and use jax.device_put to distribute it across devices:\n",
    "y = jax.device_put(x, sharding.reshape(4,2))\n",
    "jax.debug.visualize_array_sharding(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharding.reshape(4,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
